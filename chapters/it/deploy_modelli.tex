\chapter{Deploy dei modelli ML}

Una volta che è stato effettuato correttamente il training e la creazione di un modello
machine learning, arriva il momento di distribuirlo e di utilizzarlo effettivamente
per il suo scopo, ovvero quello di classificatore di tipi di parcheggio.\\
L'idea generale del funzionamento consiste nel raccogliere dati durante il tragitto
dell'auto e in seguito fornirli come input al modello. Chiaramente, i dati che 
il modello prodotto riceve in input devono essere della stessa forma di quelli
utilizzati per fare il training. Ciò implica che tutte le procedure di pulizia
dei dati e di miglioramento delle feature debbano essere ripetute ogni volta che
nuovi dati vengono raccolti per essere sottoposti al modello ed essere 
classificati.\\
Garantire che un modello riceva in input dati di forma e qualità identiche a
quelle del dataset di training può talvolta risultare laborioso. Ciò è causato 
dal fatto che gli ambienti di raccolta dati e quelli di produzione possono 
differire sotto svariati aspetti. Infatti, non è scontato che in queste due
situazioni si utilizzino lo stesso linguaggio di programmazione, lo stesso sistema
operativo, le stesse librerie, gli stessi approcci, ecc. Al contrario, è
molto probabile che un modello venga istruito una volta, per poi essere 
distribuito ed utilizzato su piattaforme diverse. In queste situazioni si può
andare incontro a numerose complicazioni, come dover ri-progettare alcuni
algoritmi a causa di un cambiamento di linguaggio di programmazione, che 
renderebbe l'algoritmo stesso meno efficente o addirittura non più funzionante.
In questo caso, è fondamentale avere una conoscenza approfondita del funzionamento
dell'algoritmo originale e della buona documentazione da poter consultare. 
Qualsiasi minimo errore di trascrizione o di comprensione potrebbe generare
modifiche sostanziali ai dati che verranno processati, rendendoli così differenti
da quelli provenienti dal training set e non più validi per una potenziale
classificazione. Spesso si può anche andare incontro ad un cambiamento di librerie,
soprattutto se si sta cambiando linguaggio di programmazione. La questione che
riguarda le librerie è ancora più delicata rispetto a quella dei linguaggi. Nella
maggior parte dei casi, esse contengono delle logiche interne che risultano molto
complesse da riprodurre o imitare in un ambiente differente. Per questo motivo,
è bene ridurre al minimo l'utilizzo di librerie esterne quando ci si trova in 
situazioni come questa, ovvero scrivendo codice che dovrà essere portato su altre
piattaforme. Tuttavia, a volte è necessario utilizzare alcune librerie, come nel
nostro caso, in cui si sono dovute utilizzare libreria per l'algebra lineare.
Quindi, anche per questo discorso occorre avere massima prudenza nella scelta di
un sostituto valido e che non generi inconsistenze nei dati.

\section{Necessità del calcolo in locale}

Per quanto riguarda il deploy di un modello, un'altra scelta da prendere è se 
eseguire il modello in un applicativo lato client, oppure su un server remoto.
Questa decisione dipende da molti fattori, tra i quali vi è la potenza di calcolo
del dispositivo che esegue l'applicativo client, potenziali requisiti di esecuzione
real-time (con bassa latenza), disponibilità del dispositivo di una connessione 
ad internet, sensibilità dei dati, ecc.\\
Entrambi gli approcci hanno vantaggi e svantaggi. Tra i vantaggi di una
 esecuzione lato server possiamo notare: 
\begin{itemize}
    \item la possibilità di sostituire il modello con una versione più aggiornata
    senza dover per forza rilasciare un nuovo aggiornamento dell'applicativo client
    \item poter effettuare l'esecuzione di un modello al posto di un dispositivo
   	con scarsa potenza di calcolo (es. un dispositivo embedded)
\end{itemize}
Invece tra quelli di una esecuzione lato client:
\begin{itemize}
    \item non dover dipendere da un server remoto, quindi da una connessione 
    stabile 
    \item evitare di dover attendere una risposta dal server, che può impiegare
    molto più tempo che una esecuzione in locale (nel caso in cui la potenza di
    calcolo del dispositivo sia sufficente)
    \item mantenere privati i dati che sono utilizzati come input del modello
    (nel caso in cui i dati siano sensibili)
\end{itemize}
Nel caso del nostro modello, è stato deciso di eseguire la predizione del modello
in locale, nel lato client dell'app GeneroCity. Questa scelta è stata presa 
principalmente per due motivi:
\begin{itemize}
    \item in prospettiva di un utilizzo dell'app da parte di numerosi utenti,
    evitare che il server backend venga sottoposto a un carico troppo oneroso,
    causato dalla potenziale esecuzione di un numero molto elevato di 
    classificazioni in parallelo;
    \item sfruttare il \emph{Neural Engine} dei dispositivi Apple, attraverso la 
    libreria \emph{CoreML} (solamente per la versione iOS di GeneroCity);
\end{itemize}

Dal momento che il processo di classificazione, attraverso un modello, è 
abbastanza intensivo e inoltre richiede la pulitura dei dati preliminare,
permettere che venga eseguito su un server potrebbe generare rallentamenti.
Questo si verifica soprattutto se il programma backend non è ditribuito.
Oltre a ciò, vi è il fatto che l'esecuzione del modello avviene in seguito
ad un'azione dell'utente (ovvero al parcheggio dell'auto) e nel caso in cui 
ci fossero degli utenti malintenzionati, si correrebbe il rischio di
ricevere troppe richieste allo stesso tempo.

\subsection{Predizione in locale con CoreML}

Dovendo effettuare la classificazione in locale, su piattaforma iOS, la 
libreria che è risultata più adatta ai nostri scopi è stata \emph{CoreML}.
Questa libreria da la possibilità di integrare modelli machine learning 
direttamente all'interno delle app. Oltre a fornire dei modelli già 
addestrati e pronti all'uso (tra cui uno per il riconoscimento di immagini,
un altro per la trascrizione audio-testo, ecc.), essa offre dei modi per 
utilizzare modelli creati direttamente dallo sviluppatore.\\
Innanzitutto, \emph{CoreML} richiede che i modelli proposti dallo
sviluppatore siano del formato proprietario ".mlmodel". Questo formato,
infatti, è stato creato proprio da Apple. Ci sono principalmente due 
modi per ottenere un modello di questa tipologia:
\begin{itemize}
    \item crearlo direttamente attraverso la libreria \emph{CreateML};
    \item convertirne uno di un altro formato (es. TensorFlow) attraverso 
    un apposito strumento di conversione;
\end{itemize}
Siccome per iniziare è stato integrato il modello soltanto nella versione
iOS dell'app, è stata scelta la prima strada, ovvero generarlo con
\emph{CreateML}. In questo modo non è stato necessario utilizzare
strumenti più personalizzabili, ma molto più complessi e a basso
livello, come TensorFlow.\\
La scelta di \emph{CoreML} è stata ovvia, in quanto su iOS è l'unica
libreria che permette di sfruttare al meglio l'hardware del dispositivo.
Infatti, essa è stata progettata proprio per essere utilizzata in
accoppiata con l'hardware GPU presente negli stessi dispositivi Apple.
Inoltre, nei dispositivi delle generazioni più recenti è stato inserito
un componente dedicato esclusivamente all'esecuzione di algoritmi di
intelligenza artificiale e modelli ML, chiamato \emph{Neural Engine}.
Esso si tratta di una sezione della GPU che è provvista di una 
architettura ad hoc e che viene sfruttata quando vengono invocate delle
predizioni attraverso \emph{CoreML}. Questa sezione è stata aggiunta 
a causa di una presenza del machine learning all'interno delle app
che è aumentata drasticamente negli ultimi anni e che con ogni
probabilità continuerà a diffondersi. Grazie a questo componente,
l'esecuzione di una predizione di machine learning (es. la 
classificazione del tipo di parcheggio) non va ad influire sulle
performance del sistema (sul resto dell'hardware) e quindi non 
crea rallentamenti o problemi visibili dall'utente.\\
Avendo deciso di eseguire il modello in locale, diventa obbligatorio
effettuare anche le fasi preliminari di preparazione dei dati in 
locale. Quindi, l'intera procedura, che va dal parcheggio 
dell'auto fino alla predizione del tipo di parcheggio, viene
eseguita all'interno dell'app iOS, senza alcun sostegno di 
servizi esterni.

\section{Porting del processore di dati}

Come già anticipato, la scelta di eseguire la classificazione del tipo di
parcheggio in locale nell'applicazione implica delle conseguenze.
Una delle più importanti tra queste è certamente il fatto di dover
pulire e preparare i dati direttamente in ambiente iOS. Ciò
ha reso obbligato un porting dello script dedicato a processare i
dati delle registrazioni, che è scritto in Python. Il linguaggio di 
destinazione del porting è Swift, ovvero il linguaggio utilizzato
per sviluppo di GeneroCity iOS.\\
Python e Swift differiscono sotto molti aspetti: oltre ad avere una sintassi
significativamente diversa, sono stati progettati per scopi abbastanza 
differenti e i loro ambienti di esecuzione non hanno molti fattori 
in comune. Mentre Python è pensato per essere utilizzato su una vasta
varietà di sistemi operativi e architetture, offrendo un interprete
con ampia compatibilità, Swift ha una target più ristretto, che 
mira principalmente ai dispositivi con sistemi operativi Apple.
Tuttavia, la differenza maggiore sta nel fatto che Python si tratta
di un linguaggio interpretato, mentre Swift è un linguaggio compilato. 
Il secondo, quindi, ha bisogno di un compilatore apposito che generi
i binari che infine vengono eseguiti.\\
Essendo due linguaggi molto utilizzati e con un background notevole,
entrambi sono dotati di un solida fornitura di librerie. Infatti, 
considerando sia librerie native, che librerie di terze parti,
Python e Swift possono essere sfruttati in moltissimi ambiti.
Per quanto riguarda il nostro obiettivo, è stato necessario
utilizzare operazioni di algebra lineare e fortunatamente entrambi i
linguaggi sono muniti di librerie mature abbastanza da permettere di 
svolgere tali operazioni in maniera sufficentemente intuitiva.\\
Ad ogni modo, la trascrizione del codice stesso non è stata affatto
priva di complicazioni. Sono state notate numerose differenze tra i
linguaggi, che hanno reso il porting una operazione tutt'altro che 
lineare:
\begin{itemize}
    \item strutture sintattiche: i due linguaggi differiscono alla base,
    nel modo di gestire gli scope dei blocchi di codice (python utilizza
    l'indentazione, mentre Swift utilizza uno stile a parentesi graffe
    C-like) e in molte strutture, come le dichiarazioni di cicli, 
    esecuzioni condizionali, operatori booleani, ecc.
    \item presenza di variabili e costanti: Swift necessita che venga 
    specificata l'entità di una variabile (se si tratta di una variabile
    o di una costante) al momento della sua dichiarazione, mentre in Python
    sono già tutte semplici variabili di default. Inoltre Swift permette
    di aggiungere molte altre keyword (es. modificatori di accesso) alle
    dichiarazioni di variabili, in modo modificare le loro proprietà.
    \item tipizzazione: mentre Python appartiene al gruppo di linguaggi
    tipizzati dinamicamente (ovvero, che permettono ad una stessa
    variabile di assumere tipi diversi all'interno dello stesso scope
    di esecuzione), Swift è tipizzato staticamente (cioè, non permette
    che una variabile cambi il proprio tipo, nello stesso scope, durante
    una esecuzione). Questo implica anche che in molti contesti, in Swift,
    bisogni esplicitamente definire il tipo richiesto (ad esempio nella
    dichiarazione dei parametri e del valore di ritorno di una funzione),
    a differenza di Python, in cui il tipo non viene mai definito. 
    \item strutture dati: non è immediato riportare molte strutture ad alto 
    livello che si trovano su Python (es. la \emph{List}) su un linguaggio
    come Swift. Spesso è necessario avere delle accortezze maggiori per 
    gestire dei dati ad un livello leggermente più basso rispetto a come 
    venivano gestiti su Python. % TODO: add something here and then go ahead...
    \item passaggi di parametri di funzioni per riferimento e per valore;
    \item gestione dei valori opzionali;
\end{itemize}


\section{Adattamento all'ambiente mobile (iOS)}





\subsection{Librerie differenti...}










